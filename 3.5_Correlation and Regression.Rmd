---
output: pdf_document
---
# Correlation and Covariance
Let us suppose we have the data of heights of 40 men who exercise regularly.
```{r}
weights <- c(119.5,135.0,137.1,137.4,139.0,144.1,144.2,151.0,151.3,151.8,152.6,156.3
,161.9,162.4,164.2,164.7,166.1,166.8,169.1,169.8,170.1,172.9,173.3,174.8
,175.2,175.8,176.7,179.3,186.6,189.1,191.1,193.8,198.0,201.5,204.6,209.4
,213.3,214.5,220.6,237.1)
y <- sort(weights)
weights
```

Now let us develop randomly distributed dataset for heights of 40 individuals.
```{r}
x <- sort(rnorm(40, 70, 6))# Randomly generated heights data
x
```

For easy calculation of covariance and correlation matrix, we create a matrix representation of dependent and predicting variables.
```{r}
matrix_wh <- cbind(x, y)
head(matrix_wh)
cov(x,y)
cor(x,y)

```

Scattor plotting of weights versus heights, and adding linear equation line to the graph:
```{r}
plot(x, y, xlab = "height", ylab = "weight", main = "weights and heights")
abline(lm(y~x))
```

Matrix of three variables("z" included with height "h" and weight "w" variables.) 
```{r}
z <- rnorm(40, 80, 10)
z <- sort(z)
matrix_whz <- cbind(x,y,z)
```

The three way variance, covariance matrix:
```{r}
cov(matrix_whz)# See that along the diagonal variance is generated
cor(matrix_whz)
```

# Regression

Let's take a dataset for distribution of GPA with a predictor variable "hours of study". It should be kept in mind however, irrespective of what prediction range is, that maximally attainable GPA for a student is 4.0.
```{r}
hours <- structure(list(Hours = c(10L, 12L, 10L, 15L, 14L, 12L, 13L, 15L, 
16L, 14L, 13L, 12L, 11L, 10L, 13L, 13L, 14L, 18L, 17L, 14L), 
    GPA = c(3.33, 2.92, 2.56, 3.08, 3.57, 3.31, 3.45, 3.93, 3.82, 
    3.7, 3.26, 3, 2.74, 2.85, 3.33, 3.29, 3.58, 3.85, 4, 3.5)), .Names = c("Hours", 
"GPA"), class = "data.frame", row.names = c("1", "2", "3", "4", 
"5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", 
"16", "17", "18", "19", "20"))
```

Regressing the GPA on hours of study with linear model we get the following output:
```{r}
attach(hours)
cor(Hours, GPA)
lm(GPA ~ Hours)
results <- lm(GPA ~ Hours)
summary(results)
```

Model predicted values of GPAs for given values of predictor variable are:
```{r}
predicted <- predict(results)
predicted
```

We can now compute the sum of square of regression (stated as treatment factor, otherwise), the residual sum of squares and the total sum of squares.
```{r}
cbind(predicted)
resid <- residuals(results)
cbind(GPA, predicted, resid)
SStot <- sum((GPA - mean(GPA))^2)
SStot
SSreg <- sum((predicted - mean(predicted))^2)# Regression sum of squares
SSreg
SSres <- sum((residuals(results))^ 2)
SSres

```

The total degrees of freedom are the number of pairs of observations minus 1, for `20 - 1 = 19`. The degrees of freedom for regression (Hours) are the number of variables minus 1, for `2 - 1 = 1`. The residual degrees of freedom are the number of pairs minus 2, for `20 - 2 = 18`. The total mean square is our familiar overall variance. We can calculate each mean square by dividing each sum of squares by its relevant degrees of freedom. We then can calculate an F ratio to test the significance of the overall regression by dividing the regression mean square by the residual mean square:

```{r}
MSreg <- SSreg / 1
MSreg
MSres <- SSres / 18
MSres
Fratio <- MSreg / MSres
Fratio
rsquare <- SSreg/SStot
rsquare
anova(results)
require(car)
Anova(results)# Same as that from the stats package "ANOVA()"
```

# Example: Predicting gasoline prices

Let us take a dataset based on real observation with price of gasoline fluctuating across several years on a monthly basis.
```{r}
gas_prices <- structure(list(Year = 1982:2011, Jan = c(1.358, 1.23, 1.216, 
1.148, 1.194, 0.862, 0.933, 0.918, 1.042, 1.247, 1.073, 1.117, 
1.043, 1.129, 1.129, 1.261, 1.131, 0.972, 1.301, 1.472, 1.139, 
1.473, 1.592, 1.823, 2.315, 2.274, 3.047, 1.787, 2.731, 3.091
), Feb = c(1.334, 1.187, 1.209, 1.131, 1.12, 0.905, 0.913, 0.926, 
1.037, 1.143, 1.054, 1.108, 1.051, 1.12, 1.124, 1.255, 1.082, 
0.955, 1.369, 1.484, 1.13, 1.641, 1.672, 1.918, 2.31, 2.285, 
3.033, 1.928, 2.659, 3.167), Mar = c(1.284, 1.152, 1.21, 1.159, 
0.981, 0.912, 0.904, 0.94, 1.023, 1.082, 1.058, 1.098, 1.045, 
1.115, 1.162, 1.235, 1.041, 0.991, 1.541, 1.447, 1.241, 1.748, 
1.766, 2.065, 2.401, 2.592, 3.258, 1.949, 2.78, 3.546), Apr = c(1.225, 
1.215, 1.227, 1.205, 0.888, 0.934, 0.93, 1.065, 1.044, 1.104, 
1.079, 1.112, 1.064, 1.14, 1.251, 1.231, 1.052, 1.177, 1.506, 
1.564, 1.407, 1.659, 1.833, 2.283, 2.757, 2.86, 3.441, 2.056, 
2.858, 3.816), May = c(1.237, 1.259, 1.236, 1.231, 0.923, 0.941, 
0.955, 1.119, 1.061, 1.156, 1.136, 1.129, 1.08, 1.2, 1.323, 1.226, 
1.092, 1.178, 1.498, 1.729, 1.421, 1.542, 2.009, 2.216, 2.947, 
3.13, 3.764, 2.265, 2.869, 3.933), Jun = c(1.309, 1.277, 1.229, 
1.241, 0.955, 0.958, 0.955, 1.114, 1.088, 1.16, 1.179, 1.13, 
1.106, 1.226, 1.299, 1.229, 1.094, 1.148, 1.617, 1.64, 1.404, 
1.514, 2.041, 2.176, 2.917, 3.052, 4.065, 2.631, 2.736, 3.702
), Jul = c(1.331, 1.288, 1.212, 1.242, 0.89, 0.971, 0.967, 1.092, 
1.084, 1.127, 1.174, 1.109, 1.136, 1.195, 1.272, 1.205, 1.079, 
1.189, 1.593, 1.482, 1.412, 1.524, 1.939, 2.316, 2.999, 2.961, 
4.09, 2.543, 2.736, 3.654), Aug = c(1.323, 1.285, 1.196, 1.229, 
0.843, 0.995, 0.987, 1.057, 1.19, 1.14, 1.158, 1.097, 1.182, 
1.164, 1.24, 1.253, 1.052, 1.255, 1.51, 1.427, 1.423, 1.628, 
1.898, 2.506, 2.985, 2.782, 3.786, 2.627, 2.745, 3.63), Sep = c(1.307, 
1.274, 1.203, 1.216, 0.86, 0.99, 0.974, 1.029, 1.294, 1.143, 
1.158, 1.085, 1.177, 1.148, 1.234, 1.277, 1.033, 1.28, 1.582, 
1.531, 1.422, 1.728, 1.891, 2.927, 2.589, 2.789, 3.698, 2.574, 
2.704, 3.612), Oct = c(1.295, 1.255, 1.209, 1.204, 0.831, 0.976, 
0.957, 1.027, 1.378, 1.122, 1.154, 1.127, 1.152, 1.127, 1.227, 
1.242, 1.042, 1.274, 1.559, 1.362, 1.449, 1.603, 2.029, 2.785, 
2.272, 2.793, 3.173, 2.561, 2.795, 3.468), Nov = c(1.283, 1.241, 
1.207, 1.207, 0.821, 0.976, 0.949, 0.999, 1.377, 1.134, 1.159, 
1.113, 1.163, 1.101, 1.25, 1.213, 1.028, 1.264, 1.555, 1.263, 
1.448, 1.535, 2.01, 2.343, 2.241, 3.069, 2.151, 2.66, 2.852, 
3.423), Dec = c(1.26, 1.231, 1.193, 1.208, 0.823, 0.961, 0.93, 
0.98, 1.354, 1.123, 1.136, 1.07, 1.143, 1.101, 1.26, 1.177, 0.986, 
1.298, 1.489, 1.131, 1.394, 1.494, 1.882, 2.186, 2.334, 3.02, 
1.689, 2.621, 2.985, 3.278), average = c(1.296, 1.241, 1.212, 
1.202, 0.927, 0.948, 0.946, 1.022, 1.164, 1.14, 1.127, 1.108, 
1.112, 1.147, 1.231, 1.234, 1.059, 1.165, 1.51, 1.461, 1.358, 
1.591, 1.88, 2.295, 2.589, 2.801, 3.266, 2.35, 2.788, 3.527), 
    index = 1:30), .Names = c("Year", "Jan", "Feb", "Mar", "Apr", 
"May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "average", 
"index"), class = "data.frame", row.names = c("1", "2", "3", 
"4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", 
"16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", 
"27", "28", "29", "30"))
```

Now we plot the average price of the gasoline against index, which essentially is a ordered vector representing the chronology of the data.
```{r}
str(gas_prices)
attach(gas_prices)
plot(index, average)
abline(lm(average ~ index))
```

The trend of rising gasoline prices over time does not appear to be linear in nature. Even with the visible departure from linearity, the linear relationship is statistically significant, as shown in the following regression analysis of the linear relationship between year and annual gas price.

```{r}
results_gas <- lm(average ~ index)
summary(results_gas)
```

# Fitting a Quadratic Model
Let us create the vector of squared values of indices and use the cbind() function to 
add the values to our gas_prices data frame.

```{r}
indexsq <- index ^ 2
indexsq
gas_prices <- cbind(gas_prices, indexsq)
```

We ultimately are still calculating a linear model by adding the squared term to make our model quadratic, because we are calculating the linear relationship between the observed prices and our linear combination of the index and squared index values.

```{r}
results <- lm(average ~ index + indexsq)
summary(results)
plot(results)# Produces residual diagnostic plots. All show a fairly good fit of quadratic model.
```

Let us use the predict() function to produce the predicted gasoline prices from the quadratic model.

```{r}
predquad <- predict(results)
gas_prices <- cbind(gas_prices, predquad)
plot(index, average, main = "quadratic model")
lines(predquad)
```

# Determining Confidence and Prediction Intervals

```{r}
results <- lm(average ~ index)
predlin <- predict(results)
conf <- predict(results, interval = "confidence")
pred <- predict(results, interval = "prediction")
plot(index, average)
lines(predict(results))
points(conf[, "lwr"], col = "blue", type = "l")
points(conf[, "upr"], col = "blue", type = "l")
points(pred[, "lwr"], col = "red", type = "l")
points(pred[, "upr"], col = "red", type = "l")
```

